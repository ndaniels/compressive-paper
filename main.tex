\documentclass{amsbook}

\usepackage{graphicx}
\usepackage{natbib}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{xca}[theorem]{Exercise}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\numberwithin{equation}{section}

%    Absolute value notation
\newcommand{\abs}[1]{\lvert#1\rvert}

%    Blank box placeholder for figures (to avoid requiring any
%    particular graphics capabilities for printing this document).
\newcommand{\blankbox}[2]{%
  \parbox{\columnwidth}{\centering
%    Set fboxsep to 0 so that the actual size of the box will match the
%    given measurements more closely.
    \setlength{\fboxsep}{0pt}%
    \fbox{\raisebox{0pt}[#2]{\hspace{#1}}}%
  }%
}

\begin{document}

\title{A compressed data structure with entropy-linear approximate similarity search}

%    Information for first author
\author{Y. William Yu}
%    Address of record for the research reported here
\address{Department of Mathematics, Massachusetts Institute of Technology, Cambridge, Massachusetts 02139}
%    Current address
%\curraddr{Department of Mathematics and Statistics,
%Case Western Reserve University, Cleveland, Ohio 43403}
\email{ywy@math.mit.edu}
%    \thanks will become a 1st page footnote.
\thanks{The first author was supported by a Hertz Foundation Fellowship.}

%    Information for second author
\author{Noah M. Daniels}
%\address{Mathematical Research Section, School of Mathematical Sciences,
%Australian National University, Canberra ACT 2601, Australia}
\email{ndaniels@mit.edu}
%\thanks{Support information for the second author.}

\author{David C. Danko}
%\address{Mathematical Research Section, School of Mathematical Sciences,
%Australian National University, Canberra ACT 2601, Australia}
\email{dcdanko@mit.edu}

\author{Bonnie Berger}
\email{bab@mit.edu}

%    General info
\subjclass[2010]{Primary 68W99}

\date{September 23, 2014}

%\dedicatory{This paper is dedicated to our advisors.}

\keywords{Similarity search, approximate matching, entropy scaling algorithms}

\begin{abstract}
Much like sorting is a primitive operation in computer science, similarity search (alternately, approximate matching) is a fundamental operation in data science and lies at the heart of many other problems.
Traditionally, approximate matching has been studied primarily in the context of strings under edit distance metrics (e.g. for spell-checkers).
In recent years, however, similarity search has become increasingly important for other objects and distance functions (e.g. biological genomes and alignment scores, networks and the Jaccard index).

As available data grows exponentially, algorithms that scale linearly no longer suffice.
The primary ways the literature addresses this problem (e.g. locality sensitive hashing, vector approximation, space partitioning) involve the construction of data structures that admit a more efficient search operation.
However, we note that as data increases, very often the redundancy present in the data also increases.
Existing general purpose methods do not explicitly take advantage of this phenomenon.

In contrast, in the emerging field of ``compressive genomics'', the prevailing algorithmic framework specifically takes advantage of the particularly pronounced redundancy in large biological data sets.
This has enabled orders of magnitude speedup of existing search algorithms such as BLAST and BLAT, as well as of applications using similarity search as a primitive, such as read mapping and network analysis.
The key to these improvements is the use of data structures that seemingly scale linearly with the ``entropy'' of the dataset, both in space and time.

In this paper, we present a prototypical compressed data structure for similarity search in Hamming-like spaces and prove the linear entropy-scaling result.

%%%%
% abstract from cablastx manuscript below
%%%%

A central goal of metagenomics is to categorize the microorganisms present
in a microbial sample. 
Metagenomic analysis begins with next generation
sequencing (NGS) of large amounts of read data, collected directly from the
environment, and attempts to assign taxonomic labels to short or 
partially-assembled DNA sequences.
However, this task is currently computationally 
infeasible when we do not have reference genomes onto which we can map the
reads, which is often the case due to the multi-varied nature of microbial
samples and the
small functional differences between closely-related strains.
Thus, researchers have instead turned to mapping such reads or partial
assemblies to protein databases to identify non-exact matches.
Here we present a novel technique for performing such mappings and
demonstrate its efficacy on data from the human gut microbiome.
In comparison to existing techniques, we are able to provide orders of
magnitude faster classification of metagenomic sequence data, without loss
of accuracy. 
While previous methods have been effective at analyzing metagenomic data
when a reference is known, this work makes such analysis tractable in the
absence of a reference genome, impacting genomic medicine and environmental
studies.
Our software tool caBLASTX can be downloaded at
http://csail.mit.edu/cablastx. 

\end{abstract}

\maketitle

\chapter{Introduction}
Throughout all areas of data science, scientists are being confronted by a veritable explosion of available data.
In many fields, and in particular molecular biology, this increase is exponential in nature, even outpacing Moore’s and Kryder’s laws on the respective doublings of transistors on a chip and long-term data storage density.
As such, the challenges posed by the massive influx of data cannot be solved by waiting for faster and larger capacity computers, but require instead the development of data structures and representations that exploit and simplify complexity in the data set.

Without loss of generality, let us consider the situation in which the data collected is non-uniformly distributed in a high-dimensional discrete space—note that the non-uniformness is nearly required for the distribution to admit any non-trivial inference and/or reasoning, and all data stored on a modern computer lives in some sense in a high-dimensional discrete space; in other words, this situation applies for nearly all large data sets.
Under this model, it is obvious that the `entropy'—which we will use to denote some signal of total information content—of the underlying data set is much lower than that suggested by a naïve encoding of the data.
We propose the development of novel data structures and algorithmic analysis tools to exploit that low-entropy nature such that the complexity of algorithms is measured not in total database or input size, but rather in the amount of non-redundant information present.

Such data structures can then be used to organize nearly any large data set for faster and more space-efficient analysis.
We have demonstrated this proof of concept through the case of similarity search in problems from computational biology (including BLAST searches and gene-protein disease networks), our area of expertise, but in this project, we look to formalize and generalize these approaches to be applicable to other fields.

To illustrate the power of this approach, we analyze some of our previously published work and present three new applications from from all corners of molecular biology (genomics, small molcule search, and protein structure) that use similarity search, where the use of entropy-linear data structures—those that scale linearly in the entropy of the data— has exhibited several orders of magnitude better performance than the previously-employed methods.

Much like sorting is a primitive operation in computer science, similarity search (alternately, approximate matching) is a fundamental operation in data science and lies at the heart of many other problems.
Traditionally, approximate matching has been studied primarily in the context of strings under edit distance metrics (e.g., for spell-checkers) [Ukkonen, 1985].
In recent years, however, similarity search has become increasingly important for other objects and distance functions (e.g., biological genomes and alignment scores, networks and the Jaccard index) [Altschul et al, 1990; Kent, 2002; Schaeffer, 2007].

As available data grows exponentially (e.g., genomic data in Figure 1), algorithms that scale linearly no longer suffice.
The primary ways the literature addresses this problem (e.g.
locality sensitive hashing [Indyk \& Motwani, 1998], vector approximation [Ferhatosmanoglu et al, 2000], space partitioning [Weber, Schek, Blott, 1998]) involve the construction of data structures that admit a more efficient search operation.
However, we note that as data increases, very often the redundancy present in the data also increases.
Existing general purpose methods do not explicitly exploit this phenomenon.

Figure 1: Genomic data available has grown at a faster exponential rate than computer processing power.

In contrast, in the emerging field of “compressive genomics,” which PI Berger and her students invented, the prevailing algorithmic framework specifically takes advantage of the particularly pronounced redundancy in large biological data sets.
Let us view the r-nearest neighbors search as the construction of a bipartite graph between queries and a database, with edges for all pairs within distance r.
The naïve approach for solving this problem in genomic local alignment using existing search algorithms such as BLAST and BLAT then takes O(mn) time, where m is the number of queries and n is the database size.

On the other hand, PI Berger’s more sophisticated approach using link pointers and edit scripts to a unique database of sequences enables orders of magnitude speedup of existing genomic search algorithms such as BLAST and BLAT [Loh, Baym, Berger, 2012] (Figure 2).
By generalizing that approach to generic similarity search, not only is the current proposal of data structures that scale with entropy a theoretical possibility, but also a practical one as evidenced by production quality implementations that are publicly available (http://cast.csail.mit.edu).
The key to these improvements is the use of data structures that seemingly scale linearly with the “entropy” of the data set, both in space and time.

\section{Random text}

Faster approximate string matching over compressed text \cite{navarro2001faster}.
Approximate Searching on Compressed Text \cite{perez2005approximate}.
A Text Compression Scheme That Allows Fast Searching Directly In The Compressed File \cite{Manber93atext}.
\begin{itemize}
\item Approximate search on compressed data is designed for a linear text, and is specific to edit distances.
\end{itemize}

Adaptive Cluster Distance Bounding for High-Dimensional Indexing \cite{ramaswamy2011adaptive}
\begin{itemize}
\item I don't think their method works well for Hamming-like distances, as
   their hyperplane separation method seems like it ought to work best
   in Euclidean-like spaces? They certainly only use very continuous
   spaces in their examples.
\item Their "compressed feature vectors" don't actually have anything to do
   with entropy, as it looks like a constant binning.
\item Because their distances are Euclidean, they didn't (and most likely
   couldn't) make a connection we're making between clustering and
   entropy/compression. Thus, their data structure still uses $O(n)$
   space, while ours can use $O(H(n))$ space, where $H(n)$ is the "entropy"
   of the dataset.
\item They didn't do a full analysis of how much clustering helps them,
   though they mention that it does and have empirical experiments.
\item They are working on the k-nearest neighbor problem, not the
   $\epsilon$-nearest neighbors problem that we are. However, their method
   certainly can be adapted to $\epsilon$-nearest neighbor, nearly
   trivially.
\end{itemize}

\chapter{Results}
\section{Entropy-scaling data structure for similarity search}

Here we provide the setup for entropy-centric analysis.
``Entropy'' is of course a characteristic of a distribution, not a data set.
More precisely, we use ``entropy'' here as a marker of the number of bits needed to encode a new point given an existing set of points.


Assume that the number of bits necessary to encode a new point is $\theta(d)$, where $d$ is the ‘distance’ to the nearest neighbor in the set.
Given the additional assumption that the data cloud is sparse, this distance assumption is clearly true the majority of the time for Hamming distance, and can also be shown for edit distances and more general similarity measures.

Then let us consider the spanning trees of the complete distance graph between points in the set.
The number of bits needed to encode the entire set according to the relationships specified in the spanning tree is then linear in the weight of that tree.
Thus, the weight of a spanning tree corresponds to the entropy of the distribution over all data sets that are in accordance with the pairwise distances specified in that tree.
One measure of the entropy of a data set would be the entropy of this distribution over a minimum weight spanning tree of the complete distance graph.

Alternately, as the above can be expensive to compute, we can also specify a set of k privileged points using which we encode all other points.
With the additional selection of an arbitrary root node from those privileged points, this encoding corresponds to a 2-level spanning tree of the complete distance graph, with pre-selected branches, but leaves selected to minimize total weight.
The distribution over all data sets in accordance with this 2-level spanning tree has a strictly higher entropy than the one corresponding to the minimum spanning tree of course, but this quantity is easier to work with for our next analysis.
Generally, entropy-scaling data structures and algorithms are data structures whose time- and space- complexity can be bounded by entropy.

Note that entropy-scaling data structures are distinct from both succinct data structures and compressed data structures.
Succinct data structures are ones that use space close to the information-theoretic limit in the worst case while permitting efficient queries; i.e.
succinct data structures do not depend on the actual entropy of the underlying data set, but have size-dependence on the potential worst-case entropy of the data set [Jacobson, 1999].
Compressed data structures, on the other hand, bound the amount of the space used by the entropy of the data set while permitting efficient queries [Grossi \& Vitter, 2005; Ferragina \& Manzini, 2000], much like entropy-scaling data structures.
Unlike entropy-scaling data structures though, they do not measure query time in terms of the entropy.
The primary theoretical advance of entropy-scaling data structures is that they bound both space and time as functions of the data set entropy.


Figure 3: Compressive acceleration of similarity search.
(a) The naïve approach tests each query against each database entry to find entries within distance  of the query.
(b) By selecting appropriate cluster centers with maximum radius  to partition the database, we can (c) first do a coarse search to find all cluster centers within distance  of a query, and then the (d) triangle inequality guarantees that a fine search over all corresponding cluster entries will suffice.

Proposed entropy-centric analysis of compressed similarity search

Note that this compression accelerated similarity search data structure can be thought of as a 2-level spanning tree, with each cluster being a branch of the tree.
Given that our distance function is bounded by the greater of the pairwise marginal entropies between two points, this tree is almost exactly the one used in the 2-level spanning tree definition of data set entropy given in our proposed framework.
Trivially then, by encoding entries within clusters by their differences from the cluster center, we see that the memory required by the data structure is proportional to entropy—in practice, it is often just as, if not more, space-efficient to simply gzip each cluster individually.
Proving time-complexity of similarity search as a function of entropy takes rather more work though.

Here we also formalize the strong structural assumption we need on the database: intuitively, we want the fractal dimension d of the database to be low, so that increases to a search radius do not significantly blow up the number of hits.
Given the connections to entropy, one might think to use the upper Minkowski dimension (also known as the entropy dimension) [Falconer, 2013].
Recall the definition of upper Minkowski dimension of a set  using the intrinsic ball-covering number:



where  is the minimal number of open balls of radius ϵ with centers in  needed to cover .
Unfortunately, as the database is finite and the space might be discrete, the given definition always results in .
Thus, we need a more appropriate definition while still capturing the dimensional scaling properties of the upper Minkowski dimension.

One commonly used and more easily measured fractal dimension that is more easily adapted for discrete spaces is the correlation dimension [Grassberger \& Procaccia, 1983].
Define the correlation integral



where  is the number of pairs of points within distance  of each other.
Then the correlation dimension  is defined by  for small .
We adapt this for our database by considering , where  is our search radius, and stopping the limit at .
The strong structural assumption is simply that  is a small constant.

We are now finally ready to analyze the performance of similarity search on this data structure.
Recall that  entries are selected as cluster centers for partitioning the database to result in clusters with maximum radius .
We will set  .
Given any query , the coarse search over the clusters always requires  operations.
Then by the triangle inequality, only clusters with centers within distance  from  contain entries within distance  of .
Because we assumed a small fractal dimension , in expectation, only a constant number of clusters will need to be checked.
Thus, non-cluster centers will only have to be checked  proportion of the time, for some constant .
Thus, expected search time is  for coarse + fine search time.

As we insert non-duplicate entries, search time increases with entropy (defined as mentioned as the weight of the 2-level distance spanning tree):

Points that fall into existing clusters increase entropy by  and search times by .

Points that do not fall into existing clusters increase entropy by  and search time by .

Since we are characterizing search performance with respect to entropy, we would like to use increases in entropy to pay for the increased search times in the style of amortized analysis.
In the second case, we obviously get enough entropy to pay for the increased search time.
However, in the first case, we get  entropy, but increase search times by , which is a problem because an adversary could force the increased entropy to be arbitrarily close to zero, breaking the entropy scaling guarantee.
Conversely, an adversary could also add arbitrarily large amounts of entropy, which would greatly increase runtime, but which would not break the entropy scaling guarantee because the added entropy would pay for the increased runtime.
Luckily, Mother Nature is not adversarial in either way, so we can often make the additional assumption that the new entries fall randomly inside the ball surrounding the cluster center, and thus the expected additional entropy from adding to an existing cluster is .
Therefore, we have enough entropy to pay for the increases in search time, and similarity search here is linear in entropy.

Caveats and Extensions

We had to make several strong assumptions on the structure of the database, most notably the low fractal dimension assumption, which kept the number of clusters that need to be searched from blowing up with .
It would be nice if we could extend the analysis to do without that assumption and instead work directly with the entropy of the database.
One approach for doing so would be to bound fractal dimension by entropy, which is not unreasonable given that the numerator in the definition of upper Minkowski dimension corresponds in a sense to the number of bits needed to specify position within a set to within radius ϵ.

Additionally, although d is assumed to be small, the number of clusters that need to be searched still grows with .
Empirical results have shown that not all of those clusters actually need to be searched to get the vast majority of correct results.
This makes sense, as the further away a cluster is from the query, the smaller the number of hits in that cluster.
For cost reasons, actual implementations such as the one given for compressively accelerated BLAST and BLAT do not search all clusters within distance , but rather some smaller radius, and still manage to get >99% of the desired hits.
Formalizing this notion, as we plan to do in this project, while unnecessary theoretically, is of tremendous practical value.

Furthermore, as mentioned before, a strict metricity assumption is not needed on the distance function.
So long as the distance function is almost a metric in the precise sense that the triangle inequality holds for most pairs of points, this kind of data structure will work for the majority of desired search hits.

Lastly, this entire section was an illustration of the application of our entropy-scaling theoretical framework only to similarity search.
Although similarity search is found as a primitive through-out data science, it is by no means the only primitive query type.
Augmenting this data structure or designing new data structures to permit other query types while maintaining theoretical guarantees will necessitate the development of new analysis tools, beyond the ones show-cased in this section.



\section{Compressive Metagenomics}

Metagenomics, the study of genomic data sequenced directly from environmental
samples, has recently grown in popularity.
From studies of the human gut microbiome to seawater and soil samples,
metagenomics has contributed to improved understanding of how ecosystems recover
from environmental damage~\cite{Tyson:2004}, how the human gut responds to 
diet
and infection~\cite{David:2014}, and even some surprising clues as to disorders 
such as Autism Spectrum Disorder~\cite{MacFabe:2012}.

Massive amounts of metagenomic reads are generated every day from 
next-generation sequencing (NGS) machines.
Overall, the rate of NGS sequencing is growing at a greater exponential rate
than computing power~\cite{Loh}.
These metagenomic reads must be stored and analyzed to do further downstream
analysis such as abundance determination (e.g. MetaPhlAn~\cite{Huttenhower}) 
and functional characterization (e.g. PICRUSt~\cite{Huttenhower}).
 Months of computing time are often required to process data for novel, 
large-scale sequencing studies that enable us to catalog genetic variation, 
provide new insights into our evolutionary history~\cite{8}, and promise to 
revolutionize the study of cell lineages in higher organisms~\cite{9}. 
These computational challenges are at present a barrier to widespread use of 
metagenomic data throughout biotechnology, which impacts genomic medicine and 
environmental genomics~\cite{blah}.

One approach to making metagenomic analysis tractable is to
sequence only the 16S ribosomal subunit of the microbiota, which is sufficient
to identify clades of microbes present in a sample. 
However, this approach cannot detect small functional differences between 
closely-related strains. 
In order to detect such minor functional differences, tools such as BLAST can 
be used to map metagenomic sequence data onto a database of known genome
sequences.
More recently, tools such as Kraken~\cite{kraken} have provided significantly 
faster methods for this analysis.
This approach, however, requires a reference genome for any organism to be
identified.

Alternatively, BLASTX is widely used, both directly for analysis, as well as in 
pipelines such as MetaPhlAn~\cite{Huttenhower} to map reads to protein 
databases.
The advantage of the BLASTX approach is that complete reference genomes are not
required; BLASTX can identify homologs, particularly in the case where the
nucleotide sequence identity is low but translated protein sequence identity
is higher~\cite{Turnbaugh:2006, Kurokawa:2007}.
However, because BLASTX must search a protein database for possible hits for
each nucleotide read from a metagenomic sample, it is computationally intensive.
For example, \citet{Mackelprang:2011} reported that using BLASTX to map 246
million reads against the KEGG~\cite{kegg} database required 800,000 CPU hours
at a supercomputing center.
Moreover, BLASTX's run time requirements scale linearly with the size of the 
full read dataset, and each year require exponentially more runtime to process 
the exponentially growing read data. 


We present caBLASTX (compressively accelerated BLASTX), an algorithm and 
software implementation that relies on the techniques of compressive 
acceleration~\cite{Loh, Daniels} to map metagenomic reads onto a protein 
database orders of magnitude faster than BLASTX.

CaBLASTX is useful for two of the most common metagenomic analysis tasks. 
The first of these is mapping assembled or partially-assembled
nucleotide sequences onto a protein database. 
When these query sequences have
little in common with one another, caBLASTX maps each sequence separately. 
Each sequence is first searched against the compressed database with a 
relatively permissive E-value threshold, called coarse search. 
Any resulting hits may represent many original sequences, so these putative 
hits are expanded, and the search and alignment is refined with a less 
permissive E-value threshold, called fine search.

The second task caBLASTX can accelerate is that of mapping short nucleotide
reads (generated by next-generation sequencing technology) to a protein
database. In this instance, there is typically high coverage of the metagenomes
being sequenced, typically 30x-200x coverage. CaBLASTX takes advantage of this
redundancy
by compressing the read set as well, obtaining an additional speed gain that is
proportional to the amount of redundancy in the read data.


We tested caBLASTX on 200-400nt partially-assembled human gut microbiome
sequences, as well as on 100nt human gut microbiome reads at ~100x coverage.
For the partially-assembled sequences, we demonstrate run-time performance
approximately 10x faster than BLASTX, with negligible loss in sensitivity and
no loss in specificity. For the short reads, we demonstrate run-time
performance approximately 700x faster than BLASTX, with less than 5\% loss in
sensitivity and no loss in specificity.

\subsubsection*{Alphabet Reduction}

Alphabet reduction--reducing the 20-letter standard amino acid alphabet to a
smaller set, in order to accelerate search or improve homology detection--has
been proposed and implemented several times~\cite{blah}.
In particular, \citet{Murphy:2000, Peterson:2009} considered reducing the
amino-acid alphabet to 17, 10, or even 4 letters.
More recently, \citet{Zhao:2012} and \citet{Huson:2013} applied a reduction to
a 4-letter alphabet, termed a ``pseudoDNA'' alphabet, in sequence alignment.

In this work, we extend the compression approach of \citet{Daniels:2013} using
alphabet reduction.
We use the alphabet reduction of \citet{Murphy:2000} to map the standard amino
acid alphabet (along with the four common ambiguous letters ) onto a 4-letter 
alphabet.
Specifically, we map F, W, and Y into one cluster; C, I, L, M, V, and J into
a second cluster, A, G, P, S, and T into a third cluster, and
D, E, N, Q, K, R, H, B, and Z into a fourth cluster.

\subsubsection*{Database Compression}

Given a protein sequence database to be compressed, we proceed as follows:
\begin{enumerate}
        \item First, initialize a table of all possible $k$-mer ``seeds'' of
        our 4-letter reduced alphabet, as well as a ``coarse'' database of
        reduced-alphabet sequences, initially containing the alphabet-reduced
        version of the first sequence in the input database.
        %
        \item For each $k$-mer of the first sequence, store its position in the
        corresponding entry in the seed table.
        %
        \item For each subsequent sequence $s$ in the input, slide a window of 
        size $k$ along the sequence, skipping single-residue repeats of length
        greater than 10.
        %
        \item Look up these $k$ residues in the seed table.
        For every entry matching to that $k$-mer in the seed table, follow
        it to a corresponding subsequence in the coarse database and attempt
        \textit{extension}.
        If no subsequences from this window can be extended, move the window
        by $m$ residues, where $m$ defaults to 10.
        \item If a match was found via extension, move the $k$-mer window to
        the first $k$-mer in $s$ after the match, and repeat the extension
        process.
\end{enumerate}
        
Given a $k$ match between sequence $s$ and a subsequence $s'$ pointed to by the
seed table, first attempt \textit{ungapped} extension:
\begin{enumerate}
        \item Within each window of 10 alphabet-reduced residues, if identical 
        4-mers in $s$ and $s'$ can be found, and at least 2 additional matching 
        residues can be found, then there is an ungapped match within that
        10-mer window between $s$ and $s'$ that exhibits at least 60\% sequence
        identity.
        \item Continue ungapped matching using 10-mer windows until no more
        10-mers of at least 60\% sequence identity are found.
        \item The result of ungapped extension is that there is an alignment 
        between $s$ and $s'$ where the only differences are substitutions,
        at least 60\% of the positions contain exact matches.
\end{enumerate}
        
When ungapped extension terminates, attempt \textit{gapped} extension.
From the end of the aligned regions thus far, align 25-mer windows of both
$s$ and $s'$ using the Needleman-Wunsch~\cite{Needleman:1970gm} algorithm using
an identity matrix.
Note that the original caBLASTP~\cite{Daniels:2013} used BLOSUM62 as it was
operating in amino acid space; as we are now operating in a reduced-alphabet
space, an identity matrix is appropriate, just as it is for nucleotide space.
After gapped extension on a window length of 25, attempt ungapped extension
again.

If neither gapped nor ungapped extension can continue, end the extension phase.
If the resulting alignment has less than 70\% sequence identity (in the 
reduced-alphabet space), or is shorter than 40 residues, discard it, and 
attempt extension on the next entry in the seed table for the original $k$-mer,
continuing on to the next $k$-mer if there are no more entries.

If the resulting alignment does have at least 70\% sequence identity in the
reduced-alphabet space, and is at least 40 residues long, then create a link
from the entry for $s'$ in the coarse database to the subsequence of $s$
corresponding to the alignment.
If there are unaligned ends of $s$ shorter than 30 residues, append them to the
match.
Longer unaligned ends that did not match any subsequences reachable from the
seed table are added into the coarse database themselves, following the same
$k$-mer indexing procedure as the first sequence.

Finally, in order to be able to recover the original sequence with its original
amino acid identities, a \textit{difference script} is associated with each
link.
This difference script is a representation of the insertions, deletions, and
substitutions resulting from the Needleman-Wunsch alignment, along with the
offset in each reduced-alphabet cluster needed to recover the original alphabet.
Thus, for example, a valine (V) is in the cluster containing C, I, L, M, V, and 
J.
Since it is the 4th entry in that 5-entry cluster, we can represent it with
the offset 4.
Since the largest cluster contains 9 elements, only four bits are needed to
store one entry in the difference script.
More balanced clusters would have allowed 3-bit storage, but at the expense of
clusters that less faithfully represented the BLOSUM62 matrix and the
physicochemical properties of the constituent amino acids.

\subsubsection*{Query Compression}

Metagenomic reads are themselves nucleotide sequences, so no alphabet reduction
is performed on them directly.
Instead, metagenomic reads are compressed using the same approach as the
protein database, without the alphabet reduction step and with a number of
different parameters.
The difference scripts for metagenomic reads do not rely on the cluster offsets,
but simply store the substituted nucleotides.
Furthermore, unlike protein databases, where most typical sequences range in 
length from 100 to over 1000 amino acids, next-generation sequencing reads are 
typically short and usually of fixed length, which is known in advance.
Thus, the minimum alignment length required for a match, and the maximum
length unaligned fragment to append to a match, require different values based
on the read length.

\subsubsection*{Search}

Given a compressed protein database and a compressed query read set, search
comprises two phases.
The first, \emph{coarse search}, considers only the coarse sequences--the
representatives--resulting from compression of the protein database and the
query set.
Just as with standard BLASTX, each coarse nucleotide read is transformed into 
each of the six possible amino acid sequences that could result from it (three 
reading frames for both the sequence and its reverse complement).
Then, each of these amino acid sequences is then reduced back to a four-letter
alphabet using the same mapping as for protein database compression.
For convenience, the four-letter alphabet is represented using the standard
nucleotide bases, though this has no particular biological significance.
This is done so that the coarse search can rely on BLASTN (nucleotide BLAST) to
search these sequences against the compressed protein database.

This coarse search uses an E-value threshold that is relaxed from the one a user
specifies for the overall search, though the user can specify this coarse 
E-value threshold.
Coarse search identifies \emph{possible} hits for each query representative from
among the representative sequences in the database.
Of course, due to the reduced alphabet, some of these hits may turn out to be
poor quality when the original amino acid alphabet is considered.
Each of these coarse hits represents one or more original sequences from the
protein database; likewise, each coarse query represents one or more original
reads from the metagenomic data set.
For each coarse query representative, the set of coarse hits is used to
reconstruct all corresponding sequences from the original database by following
links to original sequence matches and applying their difference scripts.
The resulting \emph{candidates} are thus original sequences from the protein
database, in their original amino acid alphabet.
The query representative is also used to reconstruct all corresponding sequences
from the original read set.
Thus, for each coarse query representative, there is now a subset of the
metagenomic read set (the reads represented by that coarse query) and also a
subset of the protein database (the candidates).

The second phase, \emph{fine search}, uses standard BLASTX to translate each
of these reads associated with a coarse query representative and search for
hits only in the subset of the database comprising the candidates.
This fine search phase relies on a user-specified E-value threshold (defaulting
to BLASTX's default of 10) to filter hits.
To ensure that E-value calculation is correct, BLASTX uses a corrected database
size which is the size of the original, uncompressed protein database.

\subsection*{Benchmarking}

To evaluate the run-time performance of caBLASTX, we tested it against
BLASTX in two scenarios, and against PAUDA and Kraken in one.
BLASTX is often used to align assemblies thought to be exons to a protein
database, in which case metagenomic reads have already been assembled.
In this instance, the query-side compression of caBLASTX is not applicable, so
the performance gains are more modest.
We benchmarked caBLASTX vs. BLASTX on a dataset consisting of XX assemblies
from human gut microbiota, 3.1 megabases in total, searching against the NCBI's
``NR'' non-redundant protein database.
The running time of BLASTX was XX minutes, compared to XX minutes for caBLASTX.

When aligning next-generation sequencing reads to a protein database, the
query-side compression aspect of caBLASTX becomes applicable.
We benchmarked caBLASTX against BLASTX and PAUDA on a Siberian
permafrost dataset from~\cite{blah}, which consists of XX megabases of
100-nucleotide reads.
Results appear in Table~\ref{qsbench}.



\subsection*{Validation}

Because the ground truth for any metagenomic data set is still unknown,
we took two approaches to evaluating the accuracy of caBLASTX.
First, we treated BLASTX as a gold standard; since caBLASTX accelerates BLASTX
using compression, false positives with respect to BLASTX should not be
possible, but false negatives certainly are.
We evaluated the hits from BLASTX and caBLASTX on the same human gut
assemblies used for benchmarking, in order to evaluate the accuracy when
query-side compression is not used.
For query-side compression, we also evaluated the hits from BLASTX and caBLASTX
on the Siberian permafrost dataset from~\cite{blah}.
Results appear in Table~\ref{bxacc}.

To compare accuracy PAUDA, we relied on simulated data so that the
underlying ground truth was known.
We began with 1000 bacterial protein sequences from NR, and their corresponding
nucleotide sequences from Genbank.
We generated simulated 100-nucleotide and 300-nucleotide single-end reads from
the nucleotide sequences at 100x coverage with error rates of 0\%, 1\%, and 5\%.
We then used caBLASTX and PAUDA to search NR for matching protein sequences,
and evaluated the accuracy of classification.
Results appear in Table~\ref{pacc}.

\subsection*{Discussion}

We have introduced a compression-accelerated search algorithm that boosts the
speed of BLASTX while maintaining its accuracy.
Our approach scales sublinearly with both the size of the database being
searched, and the size of the query set when analyzing metagenomic reads.

One notable contrast to metagenomic classification tools such as PAUDA and
Kraken is that caBLASTX does not require a complete reference genome for the
targeted organisms.
CaBLASTX relies on a protein database, such as NCBI's ``NR,'' but there are
individual protein sequences for many organisms present in this database whose
complete genomes are not available.
In this sense, caBLASTX is also capable of identifying species in metagenomic
samples when those species cannot be \emph{a priori} narrowed down to a small
set of genomes.

An area of particular interest to human health is \emph{functional 
metagenomics}, the analysis of metabolic pathways present in a microbial
community at large.
One approach to functional metagenomics is to first identify clades of
microbes present, and then to consider their metabolic pathways~\cite{blah}.
Another approach is to search a protein database to identify proteins involved
in known metabolic pathways.
CaBLASTX is particularly useful here, as it can accelerate the search for
homologous, or even remotely homologous proteins that may be likely to be
involved in metabolic pathways.
This capability gives caBLASTX capabilities beyond those of tools, that require
complete reference genomes.

CaBLASTX is a drop-in replacement for BLASTX in any pipeline.
As such, it can be used to accelerate other metagenomic analysis tools, such
as MetaPhlAn~\cite{blah}.
Moreover, many of these analysis tools have, for performance reasons, required
users to construct a targeted protein database, which is a time-consuming and
possibly error-prone task.
However, caBLASTX makes it feasible to use the entire NCBI ``NR'' database for
analysis.

\section{Amolite: small molecule search}

\section{Fragbag: protein structure search}

\chapter{Online Methods}

\section{Novel illustration of proposed framework applied to similarity search}

Technical details and motivation

Let us provide a bit more motivation and rigorous analysis for the general entropy-scaling similarity search data structure.
Consider the cost of querying a database of size  for all entries within some distance  of some query item .
All database entries and the query are drawn from a universe  of possible elements, and we impose on the database both a sparsity assumption and a strong structural assumption (described below).
For ease of analysis, we will also assume here that the distance function is a metric, but this metricity assumption can be relaxed.

The quantitative form of knowledge we are most interested in encoding is pairwise distance from the query.
Of course, that knowledge is already implicitly available in the database through computing pairwise distance from the query as needed.
Indeed, this is the most commonly used naive method, but it turns out to be an expensive endeavor, scaling linearly with the size of the database.

One other naive approach is to pre-compute and sort all pairwise distances between database entries and potential queries into a rainbow table.
If we are given the guarantee that the space of potential queries is small, this approach might be feasible, as then a query requires only a single lookup and effectively constant time to traverse the list of sorted distances until all database entries within r of the query are recovered.
However, in practice, we do not know in advance the queries expected, and thus constructing such a table requires memory  and time , which is undesirable as the database in sparse in , so .

Unlike cryptographic hash functions (for which rainbow tables are actually used [Oechslin, 2003]), the outputs for similar queries in similarity search are strongly correlated.
We can easily take advantage of these correlations by instead storing the pairwise distances among just the database entries in a self-similarity table.
Then, if we are really lucky and the query is a database entry, we only have to do the same constant-time lookup as in the rainbow table.
Most of the time, however, we will not be so lucky, and the query will not be a database entry.


For the general case when the query is not a database entry, let us label the database entries .
WLOG, all of these are a priori equivalent; say we first compare  to  by looking at the first row of the table.
Then by the triangle inequality, all database entries within radius  of  must be within distance  of .
Thus, if  is close to , we need only check some small fraction of the database.
If  is far from , we can instead progressively try other rows, starting with , , etc.
(note that we will call  the central element of row ).
Without going into detail, it is intuitively obvious that the closer together (more redundant) database entries are, the better the performance of similarity search on this data structure, and thus scales with entropy.
However, constructing this data structure still requires memory-complexity of  and time-complexity , which we aim to reduce in this proposal.

Note that there are still three major sources of extraneous information in the pairwise distances table:

1.
We have  copies of the database, one copy for each database entry.
However, the algorithm will almost never get to the end of the table, because it will likely contain a sufficiently close starting database entry before then, so the last rows are probably unnecessary.

2.
Similarly, the last columns of each row are also probably unnecessary, as the algorithm only cares about entries that are close to the central entry of the row.

3.
The sorting information within each row is used to check only if entries fall within a particular radius and not used beyond that.

By eliminating extraneous information, we can construct a generalization of the compressively accelerated genomics local alignment data structure for similarity search (Figure 3).

More precisely, we reduce the table to  rows.
Then, we remove all the “distant” entries from every row, so that every database entry appears in only one row, the one to whose central element it is closest to.
Additionally, we do not need the full sorting information, since each entry only appears with its closest row center.
Note that the data structure is no longer a table, but rather instead a partition of the original database, thus instead of rows, we refer to clusters and cluster centers.
Say that the maximum cluster radius is .
Then the similarity search algorithm is to do a ‘coarse search’ on the cluster centers for all centers within distance  of , expand those clusters, and do a ‘fine search’ on all entries in those clusters for those entries within distance  of  (Figure 3).

\section{Cablast}

\section{Amolite}

\section{Fragbag}

\chapter{Discussion}


\bibliographystyle{plainnat}
\bibliography{main}

\end{document}

%------------------------------------------------------------------------------
% End of journal.tex
%------------------------------------------------------------------------------

