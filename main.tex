\documentclass{amsbook}

\usepackage{graphicx}
\usepackage{natbib}
\usepackage{caption}
\usepackage{subcaption}

\usepackage{color}
\usepackage{soul}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{xca}[theorem]{Exercise}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\numberwithin{equation}{section}

%    Absolute value notation
\newcommand{\abs}[1]{\lvert#1\rvert}

%    Blank box placeholder for figures (to avoid requiring any
%    particular graphics capabilities for printing this document).
\newcommand{\blankbox}[2]{%
  \parbox{\columnwidth}{\centering
%    Set fboxsep to 0 so that the actual size of the box will match the
%    given measurements more closely.
    \setlength{\fboxsep}{0pt}%
    \fbox{\raisebox{0pt}[#2]{\hspace{#1}}}%
  }%
}

\begin{document}

\title{A compressed data structure with entropy-linear approximate similarity search}

%    Information for first author
\author{Y. William Yu}
%    Address of record for the research reported here
\address{Department of Mathematics, Massachusetts Institute of Technology, Cambridge, Massachusetts 02139}
%    Current address
%\curraddr{Department of Mathematics and Statistics,
%Case Western Reserve University, Cleveland, Ohio 43403}
\email{ywy@math.mit.edu}
%    \thanks will become a 1st page footnote.
\thanks{The first author was supported by a Hertz Foundation Fellowship.}

%    Information for second author
\author{Noah M. Daniels}
%\address{Mathematical Research Section, School of Mathematical Sciences,
%Australian National University, Canberra ACT 2601, Australia}
\email{ndaniels@mit.edu}
%\thanks{Support information for the second author.}

\author{David C. Danko}
%\address{Mathematical Research Section, School of Mathematical Sciences,
%Australian National University, Canberra ACT 2601, Australia}
\email{dcdanko@mit.edu}

\author{Bonnie Berger}
\email{bab@mit.edu}

%    General info
\subjclass[2010]{Primary 68W99}

\date{September 23, 2014}

%\dedicatory{This paper is dedicated to our advisors.}

\keywords{Similarity search, approximate matching, entropy scaling algorithms}

\begin{abstract}
Much like sorting is a primitive operation in computer science, similarity
search (alternately, approximate matching) is a fundamental operation in data
science and lies at the heart of many other problems.
Traditionally, approximate matching has been studied primarily in the context
of strings under edit distance metrics (e.g. for spell-checkers).
In recent years, however, similarity search has become increasingly important
for other objects and distance functions (e.g. biological genomes and alignment
scores, networks and the Jaccard index).

As available data grows exponentially, algorithms that scale linearly no longer
suffice.
The primary ways the literature addresses this problem (e.g. locality sensitive
hashing, vector approximation, space partitioning) involve the construction of
data structures that admit a more efficient search operation.
However, we note that as data increases, very often the redundancy present in
the data also increases.
Existing general purpose methods do not explicitly take advantage of this
phenomenon.


In contrast, in the emerging field of ``compressive genomics'', the prevailing
algorithmic framework specifically takes advantage of the particularly
pronounced redundancy in large biological data sets.
This has enabled orders of magnitude speedup of existing search algorithms such
as BLAST and BLAT, as well as of applications using similarity search as a
primitive, such as read mapping and network analysis.
The key to these improvements is the use of data structures that seemingly
scale linearly with the ``entropy'' of the dataset, both in space and time.

In this paper, we present a prototypical compressed data structure for
similarity search in Hamming-like spaces and prove the linear entropy-scaling
result.
We also provide software implementations ....

%%%%
% abstract from cablastx manuscript below
%%%%

A central goal of metagenomics is to categorize the microorganisms present
in a microbial sample. 
Metagenomic analysis begins with next generation
sequencing (NGS) of large amounts of read data, collected directly from the
environment, and attempts to assign taxonomic labels to short or 
partially-assembled DNA sequences.
However, this task is currently computationally 
infeasible when we do not have reference genomes onto which we can map the
reads, which is often the case due to the multi-varied nature of microbial
samples and the
small functional differences between closely-related strains.
Thus, researchers have instead turned to mapping such reads or partial
assemblies to protein databases to identify non-exact matches.
Here we present a novel technique for performing such mappings and
demonstrate its efficacy on data from the human gut microbiome.
In comparison to existing techniques, we are able to provide orders of
magnitude faster classification of metagenomic sequence data, without loss
of accuracy. 
While previous methods have been effective at analyzing metagenomic data
when a reference is known, this work makes such analysis tractable in the
absence of a reference genome, impacting genomic medicine and environmental
studies.
Our software tool caBLASTX can be downloaded at
http://csail.mit.edu/cablastx. 

\end{abstract}

\maketitle

\chapter{Introduction}
Throughout all areas of data science, scientists are being confronted by a veritable explosion of available data.
In many fields, and in particular molecular biology, this increase is exponential in nature, even outpacing Moore's and Kryder's laws on the respective doublings of transistors on a chip and long-term data storage density \hl{[cite]}.
As such, the challenges posed by the massive influx of data cannot be solved by waiting for faster and larger capacity computers, but require instead the development of data structures and representations that exploit and simplify complexity in the data set.

Here, we focus our attention on similarity search (alternately, approximate matching), where the task at hand is to find all entries in some database that are `similar' to some query item.
Much like sorting is a primitive operation in computer science, similarity search is a fundamental operation in data science and lies at the heart of many other problems.
Traditionally, approximate matching has been studied primarily in the context of strings under edit distance metrics (e.g., for spell-checkers) \cite{ukkonen1985algorithms}.
However, similarity search has also demonstrated increasing importance in other domains, including local alignment of sequences, chemical graphs, and protein structures \cite{altschul1990basic, kent2002blat, schaeffer2007graph, budowski2010fragbag}.
%In recent years, however, similarity search has become increasingly important for other objects and distance functions (e.g., biological genomes and alignment scores, networks and the Jaccard index) [Altschul et al, 1990; Kent, 2002; Schaeffer, 2007].

\begin{figure}[tbp]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{computeVsData}
        \caption{}
    \end{subfigure}%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{storageVsData}
        \caption{}
    \end{subfigure}
    \caption{Genomic data available has grown at a faster exponential rate than computer processing power and disk storage.}
    \label{fig:expdata}
\end{figure}

As available data grows exponentially (e.g., genomic data in Figure 1), algorithms that scale linearly no longer suffice.
The primary ways the literature addresses this problem (e.g.
locality sensitive hashing \cite{indyk1998approximate}, vector approximation \cite{ferhatosmanoglu2000vector}, space partitioning \cite{weber1998quantitative}) involve the construction of data structures that admit a more efficient search operation.
However, we note that as data increases, very often the redundancy present in the data also increases.
Existing general purpose methods do not explicitly exploit this particular feature of biological data.

In the specific context of local alignment in genomics, however, the emerging field of ``compressive genomics'' has shown that existing tools such as BLAST and BLAT can be ``compressively accelerated'' by taking advantage of high redundancy between related genomes using link pointers and edit scripts to a database of unique sequences \cite{loh2012compressive}.
Similar results have been demonstrated for local alignment in proteomics, using much the same strategies \cite{daniels2013compressive}.
Empirically, this compressive acceleration appears to scale almost linearly in the entropy of the database, often resulting in orders of magnitude better performance.

In this paper, we generalize this approach and introduce a practical data structure for similarity search that provably scales almost linearly with entropy of the database.
Specifically, if similarity is defined by some metric-like distance function and the database exhibits both low `entropy' and low `fractal dimension', this data structure performs much better than na\"ive methods.
We will define `entropy' and `fractal dimension' more precisely later, but intuitively, they are respectively measures of the total information content and scaling behavior of number of points contained in spheres of varying radii.
Although these `entropy-linear data structures' can in principle be used to organize nearly any large data set for faster and more space-efficient analysis,
we demonstrate their applicability on similarity search problems drawn from the three major Big Data challenges in computational biology: genomics, pharmaceuticals, and protein structure \hl{(NB: reword)} \cite{marx2013biology}.

\chapter{Results}
\section{Entropy-scaling data structure for similarity search}
\subsection{Data structure}
\begin{figure}[btp]
    \centering
    \includegraphics[width=1\textwidth]{dataStructure}
    \caption{ Compressive acceleration of similarity search. %
            (a) The naïve approach tests each query against each database entry to find entries within distance  of the query. %
            (b) By selecting appropriate cluster centers with maximum radius  to partition the database, we can (c) first do a coarse search to find all cluster centers within distance  of a query, and then the (d) triangle inequality guarantees that a fine search over all corresponding cluster entries will suffice.}
    \label{fig:dataStructure}
\end{figure}

Let us now define our entropy-scaling similarity search data structure (Figure \ref{fig:dataStructure}).
For ease of analysis, we will work in a high-dimensional metric space and consider the database as a set $D$ of $n$ unique points in that metric space.
Define $B_S(q,r) = \{ p \in S | ||q-p||<r \}$. The similarity search problem is thus to compute $B_D(q,r)$ for a query $q$ and radius $r$.

The data structure in its most basic incarnation simply involves clustering the points in the set by assigning them to their nearest `cluster center'.
A set $C$ of cluster centers are chosen such that no cluster has radius greater than a user-specified parameter $r_c$.
Overloading notation a bit, we will identify each cluster with its center, so $C$ is also the set of clusters.
%We will additionally require that each cluster contain at least $\frac{n}{k \alpha}$ items, for some constant $\alpha \ge 1$ (i.e. that the clusters are balanced).
For a given similarity search query of all items within distance $r$ of a query $q$, this data structure breaks the query into coarse and fine search stages.
The coarse search is over the list of cluster centers, returning $B_C(q,r + r_c)$.
Let \[\displaystyle F = \bigcup_{c \in B_C(q,r+r_c)} c , \] the union of all the returned clusters.
Then by the triangle inequality, $B_F(q,r) = B_D(q,r)$.
Thus, a fine search over the set $F$ can return all items within radius $r$ of $q$.

We will prove in this section that this data structure allows for similarity search queries in time roughly linear (provided the fractal dimension of the database is low) to the entropy of the database
Additionally, if desired, the data structure can be stored in a compressed form that uses space also linear in the entropy of the database.
However, to do this, we will first need to build additional mathematical machinery and make precise our notions of `entropy' and `fractal dimension'.

As an aside, note that entropy-scaling data structures are distinct from both succinct data structures and compressed data structures.
Succinct data structures are ones that use space close to the information-theoretic limit in the worst case while permitting efficient queries; i.e.
succinct data structures do not depend on the actual entropy of the underlying data set, but have size-dependence on the potential worst-case entropy of the data set \cite{jacobson1988succinct}.
Compressed data structures, on the other hand, bound the amount of the space used by the entropy of the data set while permitting efficient queries \cite{grossi2005compressed, ferragina2000opportunistic}, much like entropy-scaling data structures.
Unlike entropy-scaling data structures though, they do not measure time-complexity in terms of the entropy.
\textbf{The primary theoretical advance of entropy-scaling data structures is that they bound both space and time as functions of the data set entropy.}

\subsection{Theoretical setup: entropy}
Here we provide the setup for entropy-centric analysis.
Information-theoretic ``entropy'' is of course a measure of the uncertainty of a distribution or random variable, and is not well-defined for a database.
However, the term is often used in data compression as a shorthand for the number of bits needed to encode the database, a measure of the randomness of that database.
We use ``entropy'' in the latter sense; precisely, we define the entropy of a database as the number of bits needed to encode the database under some encoding scheme.
Equivalently, the entropy of a database is the entropy measured in bits of a random variable whose instances are databases that match our database on some structural condition, which we will define later.
Of course, each encoding scheme thus results in a different definition of entropy, but this simply corresponds to the compressed size of the database under different compression methods.

Let $l$ be the number of bits necessary to encode a point $p$ in the space \textit{de novo} without any other information.
Without any compression, a set of size $n$ then requires $nl$ bits to store.
Assume that the number of bits necessary to encode a point $p$ as a function of $q$ (and vice versa) is $\theta(d) \le l$, where $d = ||p -q||$ the distance from $p$ to $q$.
Effecitvely, we are imposing the condition that the distance is a measure of entropy added by a new point (`pairwise marginal entropy').
That distance bounds pairwise marginal entropy is clearly true for Hamming distance, and can also be shown for edit distances and more general similarity measures.

Let us consider the spanning trees of the complete distance graph between points in the set.
The number of bits needed to encode the entire set according to the relationships specified in the spanning tree is then linear in the weight of that tree.
Thus, the weight of a spanning tree almost corresponds to the entropy of the distribution over all data sets that are in accordance with the pairwise distances specified in that tree.
One measure of the entropy of a data set would be the entropy of this distribution over a minimum weight spanning tree of the complete distance graph.
However, we have neglected the number of bits needed to encode the tree itself; as this is an arbitrary tree, every node must encode its parent, so the entropy of the tree itself is $n \log n$ bits.
Thus, this model of set entropy is not practical as encoding the tree is already super-linear in data set size.

Alternately, we can also specify a set of $k$ privileged points using which we encode all other points.
With the additional selection of an arbitrary (or zero) root node, this encoding corresponds to a 2-level spanning tree of the complete distance graph, with pre-selected branches, but leaves selected to minimize total weight.
The distribution over all data sets in accordance with this 2-level spanning tree has a strictly higher entropy than the one corresponding to the minimum spanning tree of course, but encoding the tree itself uses only $kl$ bits.
This model of entropy also has the advantage of being much more amenable for proving bounds about the time- and space-complexity of the similarity search data structure we introduce below.

\subsection{Theoretical setup: fractal dimension}
Here we formalize the `fractal dimension' condition we alluded to: intuitively, we want the fractal dimension $d$ of the database to be low, so that increases to a search radius do not significantly blow up the number of hits.
This is crucial because the coarse search radius is the sum of the fine search radius and the maximum cluster radius.
Roughly speaking, the number of hits returned by the coarse search radius is exponential in the fractal dimension of the database in the search radius regime, so we need fractal dimension to be low.

Given the connections to entropy, one might think to use the upper Minkowski dimension (also known as the entropy dimension) \cite{falconer2013fractal}.
Recall the definition of upper Minkowski dimension of a set $S$ using the intrinsic ball-covering number:
\[
    \dim_{Minkowski}(S) := \limsup_{\epsilon \to 0} \frac{\log N(\epsilon)}{\log 1/\epsilon}
\]

where $N(\epsilon)$ is the minimal number of open balls of radius $\epsilon$ with centers in $S$ needed to cover $S$.
Unfortunately, as the database is finite and the space might be discrete, the given definition always results in $\dim_{Minkowski}(S) = 0$.
Thus, we need a more appropriate definition while still capturing the dimensional scaling properties of the upper Minkowski dimension.

One commonly used and more easily measured fractal dimension that is more easily adapted for discrete spaces is the correlation dimension \cite{grassberger1984dimensions}.
Define the correlation integral
\[
    C(\epsilon) = \lim_{N \to \infty} \frac{g(\epsilon)}{N^2}
\]
where $g(\epsilon)$ is the number of pairs of points within distance $\epsilon$ of each other.
Then the correlation dimension $d$ is defined by $C(\epsilon) \sim \epsilon^d$ for small $\epsilon$.
We adapt this for our database by considering $\epsilon \sim r$, where $r$ is our search radius, and stopping the limit at $N \to n$.
As mentioned, we require to ensure that $d$ is a small constant.

\subsection{Proving complexity bounds}
We are now finally ready to bound the time- and space-complexity of this data structure.
Note that this data structure can be thought of as a 2-level spanning tree, with each cluster being a branch of the tree.
Given that our distance function is bounded by the greater of the pairwise marginal entropies between two points, this tree is exactly the one used in the 2-level spanning tree definition of data set entropy given previously.
Trivially then, by encoding entries within clusters by their differences from the cluster center, we see that the memory required by the data structure is proportional to entropy---in practice, it is often just as, if not more, space-efficient to simply gzip each cluster individually; or, alternately, the compression can be entirely omitted if disk space is not an issue.
Proving time-complexity of similarity search as a function of entropy takes rather more work though.

Recall that $k$ entries are selected as cluster centers for partitioning the database to result in clusters with maximum radius $r_c$.
%We will set $r_c = \theta(\frac{1}{k})$.
Given any query $q$, the coarse search over the clusters always requires $k$ operations.
Additionally, the fine search is over the set $F$, which is the union of clusters with centers within distance $r+r_c$ from $q$.
As the time-complexity of similarity search is just the total of the coarse and fine searches, this implies that the total search time is $O(k + |F|)$.

By the triangle inequality, $F \subset B_D(q,r+r_c)$,
so we can bound $|F| \le |B_D(q,r+r_c)|$.
We require that in the regime of the radii $r$ and $r+r_c$ the fractal dimension $d$ is a small constant bounded by
\[
%    d \le \log \left( \frac{n}{|F|} \cdot \frac{l}{r_c} \right) \Biggm/ \log \left( \frac{r+r_c}{r} \right),
    1 \le d \le \log \left( \frac{n}{|F|} \cdot \frac{1}{r_c} \right), 
\]
implying that
\[
    r_c \ge \frac{|F|}{n} e^d \ge \frac{|F|}{n} ,
\]
which will come into play in the entropy analysis below.
%Without expanding $d$, note that
%\[
%    \left|B_D(q, r+r_c)\right| \approx \left|B_D(q,r)\right|\left(\frac{r+r_c}{r}\right)^d % \le \left|B_D(q,r)\right| \left( \frac{n l}{|F| r_c} \right) ,
%\]
%so the total number of comparison operations is 
%\[
%    \le k + |B_D(q,r)| \left( \frac{r+r_c}{r} \right)^d 
%\]
%for the coarse + fine search, where we should recall that $B_D(q,r)$ is the output size.

Note first, however, that
\[
    \left|B_D(q, r+r_c)\right| \approx \left|B_D(q,r)\right|\left(\frac{r+r_c}{r}\right)^d  \le \left|B_D(q,r)\right| \left( \frac{n }{|F| r_c} \right) ,
\]
so the total number of comparison operations is 
\[
    \le k + |B_D(q,r)| \left( \frac{r+r_c}{r} \right)^d   \le k + |B_D(q,r)| \left( \frac{n}{|F|r_c} \right)
\]
for the coarse + fine search, where we should recall that $B_D(q,r)$ is the output size.

%Additionally, recall our balance condition that each cluster had size at least $\frac{n}{k\alpha}$ and $F$ is the union of disjoint clusters, which implies that the number of clusters making up $F$
%\[
%    |B_C(q,r+r_c)| \le |F|\cdot \frac{k\alpha}{n} \le |B_D(q,r)| \left(\frac{r+r_c}{r}\right)^d \cdot \frac{k\alpha}{n}
%\]
%Recall our balance condition that each cluster had size at least $\frac{n}{k\alpha}$, which implies that

%Non-cluster centers will only have to be checked $\frac{|F|}{n}$ proportion of the time, for $C = \left(\frac{r+r_c}{r}\right)^d$.
%Thus, expected search time is $k + \theta \left( \frac{n-k}{k} \right)$ for coarse + fine search time.

Assume that as we insert non-duplicate entries, the entries fall `randomly' in the following sense:
\begin{itemize}
    \item Points that fall into existing clusters have probability no greater than $|F|/n$ to fall into $F$.
    \item Points that fall into existing clusters fall uniformly in the region of space delineated by the cluster boundaries.
\end{itemize}
Then search time increases in expectation with entropy by the following argument:
\begin{itemize}
    \item Points that do not fall into existing clusters increase entropy by $l = O(1)$  and search time by $1$.
    \item Points that fall into existing clusters increase entropy by $\theta(r_c)$ and search times by
        \begin{itemize}
            \item $1$ if the point falls into $F$.
            \item $0$ if the point does not fall into $F$.
        \end{itemize}
        Thus, in expectation, we need an additional $|F|/n \le r_c$ search operations per point.
\end{itemize}

Since we are characterizing search performance with respect to entropy, we would like to use increases in entropy to pay for the increased search times in the style of amortized analysis.
%In the first case, we obviously get enough entropy to pay for the increased search time.
%However, in the second case, we get $O(r_c) = O\left( \frac{|F|}{n} \right)$ entropy, but increase search times by $\theta\left(\frac{1}{k}\right)$,
In both cases, we obviously get enough entropy to pay for the increased search time.
Note that without the randomness assumption, an adversary could force the increased entropy to be arbitrarily close to zero, breaking the entropy scaling guarantee.
Conversely, an adversary could also add arbitrarily large amounts of entropy, which would greatly increase runtime, but which would not break the entropy scaling guarantee so long as the fractal dimension bound held, because the added entropy would pay for the increased runtime.
Luckily, Mother Nature is not adversarial in either way, so we can often make these additional assumptions. 
Therefore, we have enough entropy to pay for the increases in search time, and similarity search here is linear in entropy.

\section*{Metagenomics}

Metagenomics, the study of genomic data sequenced directly from environmental
samples, has recently grown in popularity.
From studies of the human gut microbiome to seawater and soil samples,
metagenomics has contributed to improved understanding of how ecosystems recover
from environmental damage~\cite{Tyson:2004}, how the human gut responds to 
diet
and infection~\cite{David:2014}, and even some surprising clues as to disorders 
such as Autism Spectrum Disorder~\cite{MacFabe:2012}.

Massive amounts of metagenomic reads are generated every day from 
next-generation sequencing (NGS) machines.
Overall, the rate of NGS sequencing is growing at a greater exponential rate
than computing power~\cite{Loh}.
These metagenomic reads must be stored and analyzed to do further downstream
analysis such as abundance determination (e.g. MetaPhlAn~\cite{Huttenhower}) 
and functional characterization (e.g. PICRUSt~\cite{Huttenhower}).
 Months of computing time are often required to process data for novel, 
large-scale sequencing studies that enable us to catalog genetic variation, 
provide new insights into our evolutionary history~\cite{8}, and promise to 
revolutionize the study of cell lineages in higher organisms~\cite{9}. 
These computational challenges are at present a barrier to widespread use of 
metagenomic data throughout biotechnology, which impacts genomic medicine and 
environmental genomics~\cite{blah}.

One approach to making metagenomic analysis tractable is to
sequence only the 16S ribosomal subunit of the microbiota, which is sufficient
to identify clades of microbes present in a sample. 
However, this approach cannot detect small functional differences between 
closely-related strains. 
In order to detect such minor functional differences, tools such as BLAST can 
be used to map metagenomic sequence data onto a database of known genome
sequences.
More recently, tools such as Kraken~\cite{kraken} have provided significantly 
faster methods for this analysis.
This approach, however, requires a reference genome for any organism to be
identified.

Alternatively, BLASTX is widely used, both directly for analysis, as well as in 
pipelines such as MetaPhlAn~\cite{Huttenhower} to map reads to protein 
databases.
The advantage of the BLASTX approach is that complete reference genomes are not
required; BLASTX can identify homologs, particularly in the case where the
nucleotide sequence identity is low but translated protein sequence identity
is higher~\cite{Turnbaugh:2006, Kurokawa:2007}.
However, because BLASTX must search a protein database for possible hits for
each nucleotide read from a metagenomic sample, it is computationally intensive.
For example, \citet{Mackelprang:2011} reported that using BLASTX to map 246
million reads against the KEGG~\cite{kegg} database required 800,000 CPU hours
at a supercomputing center.
Moreover, BLASTX's run time requirements scale linearly with the size of the 
full read dataset, and each year require exponentially more runtime to process 
the exponentially growing read data. 


We present caBLASTX (compressively accelerated BLASTX), an algorithm and 
software implementation that relies on the techniques of compressive 
acceleration~\cite{Loh, Daniels} to map metagenomic reads onto a protein 
database orders of magnitude faster than BLASTX.

CaBLASTX is useful for two of the most common metagenomic analysis tasks. 
The first of these is mapping assembled or partially-assembled
nucleotide sequences onto a protein database. 
When these query sequences have
little in common with one another, caBLASTX maps each sequence separately. 
Each sequence is first searched against the compressed database with a 
relatively permissive E-value threshold, called coarse search. 
Any resulting hits may represent many original sequences, so these putative 
hits are expanded, and the search and alignment is refined with a less 
permissive E-value threshold, called fine search.

The second task caBLASTX can accelerate is that of mapping short nucleotide
reads (generated by next-generation sequencing technology) to a protein
database. In this instance, there is typically high coverage of the metagenomes
being sequenced, typically 30x-200x coverage. CaBLASTX takes advantage of this
redundancy
by compressing the read set as well, obtaining an additional speed gain that is
proportional to the amount of redundancy in the read data.


We tested caBLASTX on 200-400nt partially-assembled human gut microbiome
sequences, as well as on 100nt human gut microbiome reads at ~100x coverage.
For the partially-assembled sequences, we demonstrate run-time performance
approximately 10x faster than BLASTX, with negligible loss in sensitivity and
no loss in specificity. For the short reads, we demonstrate run-time
performance approximately 700x faster than BLASTX, with less than 5\% loss in
sensitivity and no loss in specificity.

\subsection*{Compressed representation}



\subsection*{Query Compression}

Metagenomic reads are themselves nucleotide sequences, so no alphabet reduction
is performed on them directly.
Instead, metagenomic reads are compressed using the same approach as the
protein database, without the alphabet reduction step and with a number of
different parameters.
The difference scripts for metagenomic reads do not rely on the cluster offsets,
but simply store the substituted nucleotides.
Furthermore, unlike protein databases, where most typical sequences range in 
length from 100 to over 1000 amino acids, next-generation sequencing reads are 
typically short and usually of fixed length, which is known in advance.
Thus, the minimum alignment length required for a match, and the maximum
length unaligned fragment to append to a match, require different values based
on the read length.

We note that unlike the compression of the database, which can be amortized 
over future queries, the time spent clustering and compressing the queries 
cannot be amortized.
Thus, we would not refer to the query compression as entropy-scaling, but it
still provides a practical benefit.

\subsection*{Search}

Given a compressed protein database and a compressed query read set, search
comprises two phases.
The first, \emph{coarse search}, considers only the coarse sequences--the
representatives--resulting from compression of the protein database and the
query set.
Just as with standard BLASTX, each coarse nucleotide read is transformed into 
each of the six possible amino acid sequences that could result from it (three 
reading frames for both the sequence and its reverse complement).
Then, each of these amino acid sequences is then reduced back to a four-letter
alphabet using the same mapping as for protein database compression.
For convenience, the four-letter alphabet is represented using the standard
nucleotide bases, though this has no particular biological significance.
This is done so that the coarse search can rely on BLASTN (nucleotide BLAST) to
search these sequences against the compressed protein database.

This coarse search uses an E-value threshold that is relaxed from the one a user
specifies for the overall search, though the user can specify this coarse 
E-value threshold.
Coarse search identifies \emph{possible} hits for each query representative from
among the representative sequences in the database.
Of course, due to the reduced alphabet, some of these hits may turn out to be
poor quality when the original amino acid alphabet is considered.
Each of these coarse hits represents one or more original sequences from the
protein database; likewise, each coarse query represents one or more original
reads from the metagenomic data set.
For each coarse query representative, the set of coarse hits is used to
reconstruct all corresponding sequences from the original database by following
links to original sequence matches and applying their difference scripts.
The resulting \emph{candidates} are thus original sequences from the protein
database, in their original amino acid alphabet.
The query representative is also used to reconstruct all corresponding sequences
from the original read set.
Thus, for each coarse query representative, there is now a subset of the
metagenomic read set (the reads represented by that coarse query) and also a
subset of the protein database (the candidates).

The second phase, \emph{fine search}, uses standard BLASTX to translate each
of these reads associated with a coarse query representative and search for
hits only in the subset of the database comprising the candidates.
This fine search phase relies on a user-specified E-value threshold (defaulting
to BLASTX's default of 10) to filter hits.
To ensure that E-value calculation is correct, BLASTX uses a corrected database
size which is the size of the original, uncompressed protein database.

\subsection*{Benchmarking}

To evaluate the run-time performance of caBLASTX, we tested it against
BLASTX in two scenarios, and against PAUDA and Kraken in one.
BLASTX is often used to align assemblies thought to be exons to a protein
database, in which case metagenomic reads have already been assembled.
In this instance, the query-side compression of caBLASTX is not applicable, so
the performance gains are more modest.
We benchmarked caBLASTX vs. BLASTX on a dataset consisting of XX assemblies
from human gut microbiota, 3.1 megabases in total, searching against the NCBI's
``NR'' non-redundant protein database.
The running time of BLASTX was XX minutes, compared to XX minutes for caBLASTX.

When aligning next-generation sequencing reads to a protein database, the
query-side compression aspect of caBLASTX becomes applicable.
We benchmarked caBLASTX against BLASTX and PAUDA on a Siberian
permafrost dataset from~\cite{blah}, which consists of XX megabases of
100-nucleotide reads.
Results appear in Table~\ref{qsbench}.



\subsection*{Validation}

In order to validate caBLASTX's accuracy, we treated BLASTX as a gold standard. 
Since caBLASTX accelerates BLASTX
using compression, false positives with respect to BLASTX should not be
possible, but false negatives certainly are.
We evaluated the hits from BLASTX and caBLASTX on the same human gut
assemblies used for benchmarking, in order to evaluate the accuracy when
query-side compression is not used.
For query-side compression, we also evaluated the hits from BLASTX and caBLASTX
on a set of 151-nucleotide MiSeq reads from the American Gut Project,
(ERP003820) from~\cite{blah}.
We also ran DIAMOND and RapSearch2 on this same data set.
Results appear in Table~\ref{bxacc}.

Note: Due to query-side compression, the output order from caBLASTX does not
match that of BLASTX.

\section*{High-throughput Drug Screening}

- introductory info about small-molecule search in drug repurposing and discovery
- growth of PubChem over time
- cite Girke and Thornton

\subsection*{Compression}

We introduce a framework for compressing molecular databases that use the 
popular SDF molecular structure format, such as PubChem, and for searching for 
similar molecular structures in compressed space.
We designed this compression and search framework around one of the standard 
techniques for high-throughput screening of potential drug compounds, the use 
of maximum common subgraph (MCS) to identify similar motifs among molecules.
Many existing MCS-based tools, such as SMSD~\cite{thornton} and 
fmcsR~\cite{girke} compute the maximum common subgraph on fully labeled graphs; 
vertices are labeled with chemical elements, and edges are labeled with bond 
types (such as single or double bonds).
The `f' in fmcsR, which stands for ``flexible,'' indicates that some bounded 
number of vertex or edge label mismatches are permitted, at significant 
computational cost.

In order to boost the effectiveness of clustering, we employ an approach
somewhat analogous to the reversible alphabet reduction discussed in the section
on metagenomics.
Specifically, we leave carbon atoms labeled as `C,' and we label all non-carbon
atoms as `N' (for non-carbon).
Similarly, we leave single-bond edges labeled as `1,' and we label all 
non-single bond edges as `2.'
We call this approach \emph{label simplification}

Our compression approach relies on a clustering according to structural 
similarity, after structures have had their labels simplified.
Each cluster can then be represented by a single molecular structure, along 
with pointers to \emph{difference sets} between that structure and each of the 
full molecules it represents.



\subsection*{Search}

We implemented a variant of a maximum common subgraph (MCS) algorithm, known as \emph{flexible} maximum
common subgraph (fMCS), as detailed by Cao, et al.~\cite{cao}.
Our implementation allows a user to specify whether any atom mismatches or bond-type mismatches should be 
allowed in the maximum common subgraph of two molecules.
If zero mismatches are allowed, fMCS is just MCS.

As in~\cite{loh2012compressive}, our compression-accelerated search approach relies on a two-stage process.
First, a \emph{coarse} search is performed in compressed space, by searching the coarse database.
The query molecule is unlabeled and transformed in exactly the same manner as the molecular database
during compression, and this transformed query graph is matched against the coarse database.
To preserve sensitivity, this coarse search is performed with a permissive similarity score.
The idea behind coarse search is to identify \emph{possible} hits.
Since the compressed molecule database contains only unlabeled molecular graphs, there is no need to
allow bond or atom mismatches during coarse search; the search at this level is purely MCS.

Any possible hits--molecular graphs from the compressed database whose MCS to the transformed
query molecule was within the similarity score threshold--are then decompressed, by following
pointers to the removed atom and bond information, and reconstructing the original molecules.
Typically, only a small fraction of the compressed database must be decompressed.

Next, the \emph{fine search} is performed against these decompressed possible hits.
Fine search is performed using fMCS, with user-defined parameters for bond and atom mismatches and
similarity-score threshold.

\subsection*{Benchmarking}

\subsection*{Validation}

\section*{Protein Structure Search}

Given a protein of solved (or predicted) structure but unknown function, the efficient identification
of structurally similar proteins in the Protein Data Bank is critical to function and structure prediction.
One approach to structural neighbor finding is FragBag~\cite{budowski2010fragbag}, which describes each protein as a
``bag of fragments,'' where each fragment is a small structural motif; the bag of fragments is essentially
a term frequency vector representing the number of occurrences of each structural motif within the protein.
While FragBag does not generate structural alignments the way standard aligners do, it provides an excellent
filter for computing proteins that are likely to have close alignments; those alignments could then be computed
with a standard aligner.
FragBag's accuracy has been reported as comparable to structural aligners such as STRUCTAL~\cite{blah} and
ICE~\cite{blah}.
FragBag is already quite fast, much faster than these standard aligners.
However, FragBag turns out to be an excellent proof-of-concept for entropy-scaling data structures.

We intentionally approached the application of entropy-scaling data structures to FragBag in a blind manner,
without using substantial domain-specific knowledge.
Instead, we used the very same representation (bag of fragments) and distance functions (Euclidean and cosine distances)
as FragBag, a greedy k-centers algorithm, and vector difference to generate the clustered representation.

\subsection*{Clustering}
For the cluster generation, we used a randomized greedy 2-pass approach.
First, all proteins in the PDB were randomly ordered.
Then in the first pass, proteins were selected as cluster centers if and only if they were not within a user-specified Euclidean distance $r_c$ from an existing center (i.e. the first protein is always selected, and the second if further away than $r_c$ from the first, etc.).
In the second pass, every protein was assigned to the nearest cluster center.

\subsection*{Search}
Similarity search here is performed exactly as described in the data structure section, with no modifications.
For a given search query $q$ and search radius $r$,
\begin{itemize}
    \item A coarse search is used to find all cluster centers within distance $r+r_c$ of $q$.
    \item All corresponding clusters were unioned into a set $F$.
    \item A fine search was performed over the set $F$ to find all proteins within distance $r$ of $q$.
\end{itemize}

Note that while Euclidean distance is a metric, and thus the triangle inequality guarantees 100\% sensitivity, cosine distance is not.
Empirically, however, for all of the queries we performed, we achieve $\sim 99.9\%$ accuracy (Supplementary Table).

\subsection*{Benchmarking}
We explored the increases in speed resulting from applying the entropy-scaling data structure, for both Euclidean and cosine distances (Figures).
As expected, the acceleration is highly dependent on both the search radius $r$ and the maximum cluster radius $r_c$.
Additionally, the query protein also makes a major difference in the results.
We suspect that this is due to the geometry of protein fragment frequency space being very ``spiky'' and ``star-like''.
Proteins that are near the core show very little acceleration when our data structure is used because the majority of the database is nearby, whereas proteins in the periphery can have neighbors found much more quickly.
Changing the maximum cluster radius effectively makes more proteins peripheral proteins, but at the cost of overall acceleration.

Naturally, as the search radius expands, it quickly becomes necessary to compare against nearly the entire database, destroying any acceleration.
For the cosine space in particular, note that the maximum distance between any two points is $1$, so once the coarse search radius of $r+r_c >= 1.0$, there cannot ever be any acceleration.
Similarly, once the coarse search encompasses all (or nearly all) the clusters in Euclidean space, the acceleration diminishes to 1x, and the overhead costs make the entropy scaling data structure perform worse than a naive search.
However, we are most interested in proteins that are very similar to the query, the low-radius behavior, which demonstrates varying though substantial acceleration (2x-30x).

Additionally, it is instructive to note that because of he very different geometries of euclidean vs cosine space, acceleration varies tremendously for some proteins, such as 4rhv and 1bmf, which display nearly opposite behaviors.
Whereas there is nearly 30x acceleration for 4rhv in cosine space, and the same for 1bmf in euclidean space, neither achieves better than \~2.5x acceleration in the other space.

\begin{figure}[tbp]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{assets/4rhv_benchmark_cosine}
        \caption{}
    \end{subfigure}%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{assets/1AKE_benchmark_cosine}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{assets/1BMF_benchmark_cosine}
        \caption{}
    \end{subfigure}%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{assets/1RBP_benchmark_cosine}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{assets/1b7t_benchmark_cosine}
        \caption{}
    \end{subfigure}
    \caption{Fragbag cosine benchmarking data}
    \label{fig:fragbag_cosine}
\end{figure}

\begin{figure}[tbp]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{assets/4rhv_benchmark_euclid}
        \caption{}
    \end{subfigure}%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{assets/1AKE_benchmark_euclid}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{assets/1BMF_benchmark_euclid}
        \caption{}
    \end{subfigure}%
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{assets/1RBP_benchmark_euclid}
        \caption{}
    \end{subfigure}
    \begin{subfigure}[b]{0.49\textwidth}
        \includegraphics[width=1\textwidth]{assets/1b7t_benchmark_euclid}
        \caption{}
    \end{subfigure}
    \caption{Fragbag euclidean distance benchmarking data}
    \label{fig:fragbag_euclid}
\end{figure}


\subsection*{Validation}
As here we use a single metric distance, the entropy-scaling data structure gives 100\% concordance with the na\"ive linear search.

\chapter*{Discussion}

We have introduced a compression-accelerated search algorithm that boosts the
speed of BLASTX while maintaining its accuracy.
Our approach scales sublinearly with both the size of the database being
searched, and the size of the query set when analyzing metagenomic reads.

One notable contrast to metagenomic classification tools such as PAUDA and
Kraken is that caBLASTX does not require a complete reference genome for the
targeted organisms.
CaBLASTX relies on a protein database, such as NCBI's ``NR,'' but there are
individual protein sequences for many organisms present in this database whose
complete genomes are not available.
In this sense, caBLASTX is also capable of identifying species in metagenomic
samples when those species cannot be \emph{a priori} narrowed down to a small
set of genomes.

An area of particular interest to human health is \emph{functional 
metagenomics}, the analysis of metabolic pathways present in a microbial
community at large.
One approach to functional metagenomics is to first identify clades of
microbes present, and then to consider their metabolic pathways~\cite{blah}.
Another approach is to search a protein database to identify proteins involved
in known metabolic pathways.
CaBLASTX is particularly useful here, as it can accelerate the search for
homologous, or even remotely homologous proteins that may be likely to be
involved in metabolic pathways.
This capability gives caBLASTX capabilities beyond those of tools, that require
complete reference genomes.

CaBLASTX is a drop-in replacement for BLASTX in any pipeline.
As such, it can be used to accelerate other metagenomic analysis tools, such
as MetaPhlAn~\cite{blah}.
Moreover, many of these analysis tools have, for performance reasons, required
users to construct a targeted protein database, which is a time-consuming and
possibly error-prone task.
However, caBLASTX makes it feasible to use the entire NCBI ``NR'' database for
analysis.

\chapter{Online Methods}

\section{Technical details and motivation for the similarity search data structure}
Let us provide a bit more motivation and rigorous analysis for the general entropy-scaling similarity search data structure.
Consider the cost of querying a database of size $n$ for all entries within some distance $r$ of some query item $q$.
All database entries and the query are drawn from a universe $U$ of possible elements, and we impose on the database both a sparsity assumption and a strong structural assumption (described below).
%For ease of analysis, we will also assume here that the distance function is a metric, but this metricity assumption can be relaxed.

%The quantitative form of knowledge we are most interested in encoding is pairwise distance from the query.
%Of course, that knowledge is already implicitly available in the database through computing pairwise distance from the query as needed.
%Indeed, this is the most commonly used naive method, but it turns out to be an expensive endeavor, scaling linearly with the size of the database.
The most commonly used na\"ive method is to simply compute pairwise distance from the query as needed.
This is easy, but requires a linear scan through the entire database, which itself uses also linear space.
Unfortunately, this can often be prohibitively slow.

One other naive approach is to pre-compute and sort all pairwise distances between database entries and potential queries into a rainbow table.
If we are given the guarantee that the space of potential queries is small, this approach might be feasible, as then a query requires only a single lookup and effectively constant time to traverse the list of sorted distances until all database entries within $r$ of the query are recovered.
However, in practice, we do not know in advance the queries expected, and thus constructing such a table requires memory $O(n|U|)$ and time $O(n \log n |U|)$, which is undesirable as the database in sparse in $U$, so $|U| >>n$.

Unlike cryptographic hash functions (for which rainbow tables are actually used \cite{oechslin2003making}, the outputs for similar queries in similarity search are strongly correlated.
We can easily take advantage of these correlations by instead storing the pairwise distances among just the database entries in a self-similarity table.
Then, if we are really lucky and the query is a database entry, we only have to do the same constant-time lookup as in the rainbow table.
Most of the time, however, we will not be so lucky, and the query will not be a database entry.

For the general case when the query is not a database entry, let us label the database entries $d_1, \ldots, d_n$.
WLOG, all of these are \textit{a priori} equivalent; say we first compare $q$ to $d_1$ by looking at the first row of the table.
Then by the triangle inequality, all database entries within radius $r$ of $q$ must be within distance $||q-d_1|| +r$ of $d_1$.
Thus, if $q$ is close to $d_1$, we need only check some small fraction of the database.
If $d_1$ is far from $q$, we can instead progressively try other rows, starting with $d_2, d_3$, etc.
(note that we will call $d_i$ the central element of row $i$).
Without going into detail, it is intuitively obvious that the closer together (more redundant) database entries are, the better the performance of similarity search on this data structure, and thus we begin to scale with entropy.
However, constructing this data structure still requires memory-complexity of $O(n^2)$ and time-complexity $O(n^2 \log n)$, which is still not ideal.

Note that there are still three major sources of extraneous information in the pairwise distances table:

\begin{enumerate}
    \item We have $n$ copies of the database, one copy for each database entry.
However, the algorithm will almost never get to the end of the table, because it will likely contain a sufficiently close starting database entry before then, so the last rows are probably unnecessary.
    \item Similarly, the last columns of each row are also probably unnecessary, as the algorithm only cares about entries that are close to the central entry of the row.
    \item The sorting information within each row is used to check only if entries fall within a particular radius and not used beyond that.
\end{enumerate}

It is by eliminating this extraneous information that we constructed a generalized data structure we introduced in the paper.
More precisely, we reduce the table to $k$ rows.
Then, we remove all the ``distant'' entries from every row, so that every database entry appears in only one row, the one to whose central element it is closest.
Additionally, we do not need the full sorting information, since each entry only appears with its closest row center.
Note that the data structure is no longer a table, but rather instead a partition of the original database, thus instead of rows, we refer to clusters and cluster centers.
Say that the maximum cluster radius is $r_c$.
Then the similarity search algorithm is to do a `coarse search' on the cluster centers for all centers within distance $r+r_c$ of $q$, expand those clusters, and do a ‘fine search’ on all entries in those clusters for those entries within distance $r$ of $q$, as stated earlier.

\section{Cablastx}

\subsection*{Alphabet Reduction}

Alphabet reduction--reducing the 20-letter standard amino acid alphabet to a
smaller set, in order to accelerate search or improve homology detection--has
been proposed and implemented several times~\cite{blah}.
In particular, \citet{Murphy:2000, Peterson:2009} considered reducing the
amino-acid alphabet to 17, 10, or even 4 letters.
More recently, \citet{Zhao:2012} and \citet{Huson:2013} applied a reduction to
a 4-letter alphabet, termed a ``pseudoDNA'' alphabet, in sequence alignment.

In this work, we extend the compression approach of \citet{Daniels:2013} using
a reversible alphabet reduction.
We use the alphabet reduction of \citet{Murphy:2000} to map the standard amino
acid alphabet (along with the four common ambiguous letters ) onto a 4-letter 
alphabet.
Specifically, we map F, W, and Y into one cluster; C, I, L, M, V, and J into
a second cluster, A, G, P, S, and T into a third cluster, and
D, E, N, Q, K, R, H, B, and Z into a fourth cluster.
By storing the offset within each cluster of the original letter, the original
sequence can be reconstructed, making this a reversible reduction.

\subsection*{Database Compression}

Given a protein sequence database to be compressed, we proceed as follows:
\begin{enumerate}
        \item First, initialize a table of all possible $k$-mer ``seeds'' of
        our 4-letter reduced alphabet, as well as a ``coarse'' database of
        reduced-alphabet sequences, initially containing the alphabet-reduced
        version of the first sequence in the input database.
        %
        \item For each $k$-mer of the first sequence, store its position in the
        corresponding entry in the seed table.
        %
        \item For each subsequent sequence $s$ in the input, slide a window of 
        size $k$ along the sequence, skipping single-residue repeats of length
        greater than 10.
        %
        \item Look up these $k$ residues in the seed table.
        For every entry matching to that $k$-mer in the seed table, follow
        it to a corresponding subsequence in the coarse database and attempt
        \textit{extension}.
        If no subsequences from this window can be extended, move the window
        by $m$ residues, where $m$ defaults to 10.
        \item If a match was found via extension, move the $k$-mer window to
        the first $k$-mer in $s$ after the match, and repeat the extension
        process.
\end{enumerate}
        
Given a $k$ match between sequence $s$ and a subsequence $s'$ pointed to by the
seed table, first attempt \textit{ungapped} extension:
\begin{enumerate}
        \item Within each window of 10 alphabet-reduced residues, if identical 
        4-mers in $s$ and $s'$ can be found, and at least 2 additional matching 
        residues can be found, then there is an ungapped match within that
        10-mer window between $s$ and $s'$ that exhibits at least 60\% sequence
        identity.
        \item Continue ungapped matching using 10-mer windows until no more
        10-mers of at least 60\% sequence identity are found.
        \item The result of ungapped extension is that there is an alignment 
        between $s$ and $s'$ where the only differences are substitutions,
        at least 60\% of the positions contain exact matches.
\end{enumerate}
        
When ungapped extension terminates, attempt \textit{gapped} extension.
From the end of the aligned regions thus far, align 25-mer windows of both
$s$ and $s'$ using the Needleman-Wunsch~\cite{Needleman:1970gm} algorithm using
an identity matrix.
Note that the original caBLASTP~\cite{Daniels:2013} used BLOSUM62 as it was
operating in amino acid space; as we are now operating in a reduced-alphabet
space, an identity matrix is appropriate, just as it is for nucleotide space.
After gapped extension on a window length of 25, attempt ungapped extension
again.

If neither gapped nor ungapped extension can continue, end the extension phase.
If the resulting alignment has less than 70\% sequence identity (in the 
reduced-alphabet space), or is shorter than 40 residues, discard it, and 
attempt extension on the next entry in the seed table for the original $k$-mer,
continuing on to the next $k$-mer if there are no more entries.

If the resulting alignment does have at least 70\% sequence identity in the
reduced-alphabet space, and is at least 40 residues long, then create a link
from the entry for $s'$ in the coarse database to the subsequence of $s$
corresponding to the alignment.
If there are unaligned ends of $s$ shorter than 30 residues, append them to the
match.
Longer unaligned ends that did not match any subsequences reachable from the
seed table are added into the coarse database themselves, following the same
$k$-mer indexing procedure as the first sequence.

Finally, in order to be able to recover the original sequence with its original
amino acid identities, a \textit{difference script} is associated with each
link.
This difference script is a representation of the insertions, deletions, and
substitutions resulting from the Needleman-Wunsch alignment, along with the
offset in each reduced-alphabet cluster needed to recover the original alphabet.
Thus, for example, a valine (V) is in the cluster containing C, I, L, M, V, and 
J.
Since it is the 4th entry in that 5-entry cluster, we can represent it with
the offset 4.
Since the largest cluster contains 9 elements, only four bits are needed to
store one entry in the difference script.
More balanced clusters would have allowed 3-bit storage, but at the expense of
clusters that less faithfully represented the BLOSUM62 matrix and the
physicochemical properties of the constituent amino acids.

\section{Ammolite}

\subsection*{Simplification and compression}

First, we remove hydrogen atoms from the molecules; hydrogens can be easily recovered by inferring their 
presence.
Next, each molecule's graph structure is converted to an unlabeled one; each atom label is removed, and each
bond type is represented as just a single bond.
The removed information--atom and bond type--is stored in an auxiliary data structure, indexed by the molecule's
PubChem ID and the atom or bond number.
Next, any vertex or edge that is not part of a simple cycle or a tree is removed, and any edge that is part
of a tree is removed.
This preserves the node count, but not the topology, of tree-like structures, and preserves simple cycles,
which represent rings in chemical compounds.
For example, as shown in Figure~\ref{}, bicyclohexyl and dicyclohexylamine would appear identical.

After this transformation is applied to each molecule in a database to be compressed, we identify all clusters
of fully-isomorphic transformed molecular graphs.
Isomorphism detection is performed using the VF2~\cite{Cordella} algorithm; a simple hash computed from the
number of vertices and edges in each transformed molecular graph is first used to filter molecular graphs that
cannot possibly be isomorphic.
A representative from each such cluster is stored in SDF format; collectively, these representatives form a 
``coarse'' database.
Along with each representative, we preserve the information necessary to reconstruct each original molecule,
as a pointer to a set of vertices and edges that have been removed or unlabeled.

\section{Fragbag}

\chapter{Discussion}


\bibliographystyle{plain}
\bibliography{main}

\appendix
\chapter{Extraneous}
\section{Random text}

Faster approximate string matching over compressed text \cite{navarro2001faster}.
Approximate Searching on Compressed Text \cite{perez2005approximate}.
A Text Compression Scheme That Allows Fast Searching Directly In The Compressed File \cite{manber93atext}.
\begin{itemize}
\item Approximate search on compressed data is designed for a linear text, and is specific to edit distances.
\end{itemize}

Adaptive Cluster Distance Bounding for High-Dimensional Indexing \cite{ramaswamy2011adaptive}
\begin{itemize}
\item I don't think their method works well for Hamming-like distances, as
   their hyperplane separation method seems like it ought to work best
   in Euclidean-like spaces? They certainly only use very continuous
   spaces in their examples.
\item Their "compressed feature vectors" don't actually have anything to do
   with entropy, as it looks like a constant binning.
\item Because their distances are Euclidean, they didn't (and most likely
   couldn't) make a connection we're making between clustering and
   entropy/compression. Thus, their data structure still uses $O(n)$
   space, while ours can use $O(H(n))$ space, where $H(n)$ is the "entropy"
   of the dataset.
\item They didn't do a full analysis of how much clustering helps them,
   though they mention that it does and have empirical experiments.
\item They are working on the k-nearest neighbor problem, not the
   $\epsilon$-nearest neighbors problem that we are. However, their method
   certainly can be adapted to $\epsilon$-nearest neighbor, nearly
   trivially.
\end{itemize}


\section{Caveats and Extensions}

We had to make several strong assumptions on the structure of the database, most notably the low fractal dimension assumption, which kept the number of clusters that need to be searched from blowing up with .
It would be nice if we could extend the analysis to do without that assumption and instead work directly with the entropy of the database.
One approach for doing so would be to bound fractal dimension by entropy, which is not unreasonable given that the numerator in the definition of upper Minkowski dimension corresponds in a sense to the number of bits needed to specify position within a set to within radius ϵ.

Additionally, although d is assumed to be small, the number of clusters that need to be searched still grows with .
Empirical results have shown that not all of those clusters actually need to be searched to get the vast majority of correct results.
This makes sense, as the further away a cluster is from the query, the smaller the number of hits in that cluster.
For cost reasons, actual implementations such as the one given for compressively accelerated BLAST and BLAT do not search all clusters within distance , but rather some smaller radius, and still manage to get >99% of the desired hits.
Formalizing this notion, as we plan to do in this project, while unnecessary theoretically, is of tremendous practical value.

Furthermore, as mentioned before, a strict metricity assumption is not needed on the distance function.
So long as the distance function is almost a metric in the precise sense that the triangle inequality holds for most pairs of points, this kind of data structure will work for the majority of desired search hits.

Lastly, this entire section was an illustration of the application of our entropy-scaling theoretical framework only to similarity search.
Although similarity search is found as a primitive through-out data science, it is by no means the only primitive query type.
Augmenting this data structure or designing new data structures to permit other query types while maintaining theoretical guarantees will necessitate the development of new analysis tools, beyond the ones show-cased in this section.


\end{document}

%------------------------------------------------------------------------------
% End of journal.tex
%------------------------------------------------------------------------------

